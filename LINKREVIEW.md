# LinkReview

- Here we collect all the works that may be useful for writing our paper
- We divide these works by topic in order to structure them

> [!NOTE]
> This review table will be updated, so it is not a final version.

| Topic | Title | Year | Authors | Paper | Code | Summary |
| :--- | :--- | :---: | :--- | :---: | :---: | :--- |
| Topic #1 | Paper Title | Publishing Year | Author #1 et al. | [arXiv/DOI]() | [GitHub]() | Short summary to be inserted in the Related Work section |
| SignSGD | signSGD: Compressed Optimisation for Non-Convex Problems | 2018 | Bernstein, J., et al. | [arXiv](https://arxiv.org/abs/1802.04434) | GitHub | Introduces sign-based gradient descent to reduce communication costs in distributed training. |
| SignSGD | signSGD with Majority Vote is Communication Efficient And Fault Tolerant | 2019 | Bernstein, J., et al. | [arXiv](https://arxiv.org/abs/1810.05291) | GitHub | about signSGD with Majority|
| Muon / LMO | Old Optimizer, New Norm: An Anthology | 2024 | Bernstein, J., Newhouse L. | [arXiv](https://arxiv.org/abs/2409.20325) | GitHub | Connects classical optimization methods with modern norm-based constraints and preconditioning. |
| Muon / LMO | Muon: An Optimizer for Hidden Layers in Neural Networks | 2024 | Jordan, K., et al. | Blog | [GitHub](https://github.com/KellerJordan/Muon) | Proposes Muon optimizer specifically designed for hidden layers using matrix preconditioning. |
| Muon / LMO | Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization | 2025 | Kovalev, D. | [arXiv](https://arxiv.org/abs/2503.12645) | GitHub | Provides theoretical analysis of gradient orthogonalization via Non-Euclidean Trust-Region Optimization. |
| Muon / LMO | The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization | 2025 | Kravatskiy, A., et al. | [arXiv](https://arxiv.org/abs/2512.09678) | GitHub | Explores Ky Fan norms and dual norm combinations for advanced matrix optimization tasks. |
